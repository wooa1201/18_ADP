
## 군집분석(cluster analysis)

>  * 각 개체에 대해 관측된 여러 개의 변수( $x_1$ , $x_2$, .., $x_p$ ) 값들로부터 n 개의 개체들 유사한 성격을 가지는 몇개의 군집으로 집단화하고, 형성된 군집들의 특성을 파악하여 군집들 사이의 관계를 분석하는 다변량분석 기법
>  * 이용되는 다변량 자료는 별도의 반응변수가 요구되지 않으며, 오로지 개체들간의 유사성 에만 기초
>  * 이상값 탐지에도 사용되며, 심리학,사회학,경영학,생물학 등 다양한 분야에 이용


#### 1. 계층적 군집 (hierarchical clustering)
   * 가장 유사한 개체를 묶어 나가는 과정을 반복하여 원하는 개수의 군집을 형성하는 방법
   * 보통 계통도 또는 덴드로그램(dendrogram) 형태의 결과가 주어지며 각 개체는 하나의 군집에만 속함
   * 개체간의 유사성(또는 거리)에 대한 다양한 정의가 가능하며 군집간의 연결법에 따라 군집의 결과가 달라짐 <br /><br />

   * 계층적 군집 형성 방법
   ![](https://t1.daumcdn.net/cfile/tistory/243A634B5752938B09)
     * 병합적 방법 : 작은 군집으로 출발하여 군집을 병합 (매단계마다 그룹 쌍간의 거리를 계산하여 가까운 순으로 병합 )
     * 분할적방법 : 큰 군집으로부터 출발하여 분리

   * 덴드로그랩 : 계층형 군집의 결과 표현, 군집들 간의 구조적 관계를 파악, 항목간의 거리, 군집간의 거리를 알수 있고 군집내 항목간 유사정도를 파악함으로써 군집의 견고성을 해석 <br /><br />


   * 거리측정 방법  
     * 최단연결법(단일연결법, single linkage method) : 두군집 사이의 거리를 간 군집에서 하나씩 관측값을 뽑았을 때 나타낼수 있는 거의 최소값으로 측정, 사슬모양으로 생길수 있으며, 고립된 군집을 찾는데 중점
     * 최장연결법(완전연결법, complete linkage method) : 최대값으로 측정, 같은 군집에 속하는 관측치는 알려진 최대 거리보다 짧으며, 군집들 내부의 응집성에 중점
     * 중심연결법(centroid linkage) : 군집 중심간의 거리 측정, 결합될 때 새로운 군집의 평균은 가중평균을 통해 구함
     * 평균연결법(average linkage) : 모든 항목에 대한 거리 평균, 계산량이 불필요하게 많아질수 있음
        ![http://i.imgur.com/TM1PuQc.png](http://i.imgur.com/TM1PuQc.png)

   * 모든 변수가 연속형인 경우 다양한 거리
      - 수학적 거리
         -  유클리드 거리 (Euclidian)
         -  맨하탄 거리 (Manhattan) 또는 시가(city-block)
         -  민코우스키(Minkowski)
      - 통계적 거리
         -  표준화 거리 (Standardized) : 변수의 측정단위를 표준화한 거리
         - 마할라노비스 거리 (Mahalanobis distance) : 표준화와 함께 변수간 상관성(분포형태) 동시에 고려

    * 모든 변수가 명목형인 경우 개체 i 와 j 간의 거리     
      ``` d(i, j) = (개체 i 와 j 에서 다른 값을 가지는 변수의 수 ) / (총변수의 수)  ```

        - 유사성 측도 : 단순 일치 계수(Simple Matching Coefficient) , 자카드(Jaccard)  계수
    * 순서형 자료 : 순위 상관 계수  

#### 2. k-평균군집(k-means clustering)
  > 원하는 군집수(k개)만큼 초기값을 지정하고, 각 개체를 가까운 초기값에 할당하여 군집을 형성, 각 군집의 평균을 재계산하여 초기값 갱신, 갱신된 값에 대해 할당과정을 반복하여 k 개의 최종군집 형성

 * **k-평균군집 절차**
    1. 초기 (군집의) 중심으로 k개의 객체를 임의로 선택
    2. 각 자료를 가장 가까운 군집 중심에 할당
    3. 각 군집 내의 자료들의 평균을 계산하여 군집의 중심을 갱신
    4. 군집 중심의 변화가 거의 없을때까지 2-3 반복

     ![](https://mblogthumb-phinf.pstatic.net/20160820_293/leedk1110_1471694273998eWGe7_PNG/kmeans.PNG?type=w2)

    -  군집의 수(k)는 미리 정해주어야 함
    -  k 개의 초기 중심값은 임의로 선택될수 있으나, 자료값중에서 무작위로 선택하는 것이 편리
    - 초기 중심점들은 서로 멀리 떨어져 있는 것이 바람직, 초기값에 따라 군집 결과가 달라질수 있음
    -  k-평균군집은 군집의 매단계마다 군집 중심으로부터의 오차제곱합으르 최소화 하는 방향으로 군집 형성하는 탐욕적 알고리즘으로 간주
    - 안정된 군집은 보장하나 전체적으로 최적이라는 것은 보장 못함
    <br />
    - 장점 : 단순, 빠르게 수행, 계층적 군집보다 많은 양의 자료 수행( 평균 등 거리계산에 기반하므로 모든 변수가 연속적이어야 함 )
    - 단점 : 잡음이나 이상값에 영향, 볼록한 형태가 아닌 군집에 성능 저하     
      ![](http://cfile30.uf.tistory.com/image/21235E3E5694E34E342781)


  * 단점 보완 (이상값에 민감)
     -  군집형성하는 매단계마다 평균값 대신 중앙값(k-중앙값) 사용 가능 (R : pam() 함수 이용)
     -  수행전 탐색적 자료분석을 통해 이상값 미리 제거

#### 3. 혼합 분포 군집(mixture distribution clustering)
   > - 모형 기반 (model based)의 군집방법
   > - 데이터가 k 개의 모수적모형(정규분포 or 다변량 정규분포 가정)의 가중합으로 표현되는 모집단 모형으로 부터 나왔다는 가정하에서 모수와 함께 가중치를 자료로부터 추정하는 방법을 사용
   > - k개의 각 모형은 군집의미, 각 데이터는 추정된 k개의 모형 중 어느 모형으로부터 나왔을 확률이 높은지에 따라 군집 분류
   > - 흔히 모수와 가중치의 추정(최대가능도추정)에는  EM 알고리즘 사용

   ![](https://t1.daumcdn.net/cfile/tistory/2204F74155B87EA307)
     _분포형태가 다봉형 형태로, 단일 분포로의 적합은 적절하지 않음. 3개 정도의 정규분포의 결합_
     _매우복잡한 형태의 자료에 대해 충분한 개수의 정규분포를 고려하면 모집단 분포를 잘 근사_

   * 혼합 분포에ㅓ 모수 추정은 가능도함수에 최대가능도 추정이 쉽지 않음  : 가능도함수(or 로그-가능도함수) 의 표현식이 복잡하여 미분을 토한 이론적 전개가 쉽지 않기때문 ==> EM알고리즘 이용  

    * **EM 알고리즘**
      * 모집단을 구성하는 각 집단의 분포는 정규분포를 따른다고 가정
      * 각 자료가 M 개 중에 어느 집단(class)로부터 나온것인지를 안다면 해당 모수의 추정이 어렵지 않음
      * 각 자료가 어느 집단에 속하는지에 대한 정보 잠재변수(latent variable) 도입
      * 잠재변수(Z, 모수에 대한 초기값) 각 자료의 집단 추정 가능(E-단계)
      * 각 자료에 대한 Z의 조건부분포로 부터 기댓값 구함
      * 관측변수 X 와 잠재변수 Z 의 로그-가능도함수 에 Z 대신 상수 Z의 조건부 기대값 대입하여 로그-가능도함수를 최대로 하는 모수를 쉽게 찾을 수 있다(M-단계)
      * 갱신된 모수 추정치에 대해 과정 반복, 수렴값 얻고 이를 최대가능도추정치로 사용

      |![](http://norman3.github.io/prml/images/Figure9.8a.png)|![](http://norman3.github.io/prml/images/Figure9.8b.png)|![](http://norman3.github.io/prml/images/Figure9.8c.png)|
      |-|-|-|
      |![](http://norman3.github.io/prml/images/Figure9.8d.png)|![](http://norman3.github.io/prml/images/Figure9.8e.png)|![](http://norman3.github.io/prml/images/Figure9.8f.png)|

    * 혼합분포 군집 모형 특징
      * k-평균군집의 절차와 유사, 확률분포 도입 군집을 수행하는 모형-기반의 군집 방법
      * 군집을 몇개의 모수로 표현할수 있으며, 서로 다은 크기나 모양의 군집 가능
      * EM 알고리즘을 이용한 모수추정에 데이터가 커지면 수렴하는데 시간 걸림, 군집 크기 작으면 추정의 부정확
      * 이상값 자료에 민감하므로 사전 조치 필요

#### 4.SOM(Self-Organizing Maps : 자기 조직화 지도)
> * 비지도 신경망(unsupervised neural network)으로 고차원의 데이터를 이해하기 쉬운 저차원의 뉴런으로 정렬하여 지도(map)의 형태로 형상화
> * 입력변수의 위치 관계를 그대로 보존, 실제 공간의 입력 변수가 가까이 있으면 지도상에서도 가까운 위치
> * 입력 변수의 정보와 그들의 관계가 지도상에 그래로 나타남

  ![](http://i.imgur.com/ZsAdHxT.png)

 * 입력층
      * 입력벡터를 받는 층
      * 입력변수의 개수와 동일하게 뉴런 수 존재       
 * 경쟁층  
     *  2차원 격자 구성
     * 입력벡터의 특성에 따라 벡터가 한점으로 클러스터링 되는 층
     * 사용자가 미리 정해놓은 군집의 수만큼 뉴런 존재
 * Map : 입력층 자료는 학습을 통해 경쟁층에서 정렬
 * 완전 연결(fully connected) : 입력층 뉴런은 경쟁층 뉴런과 각각 연결  

<br />

* 각 학습 단계마다 입력층 표본벡터 x 가 임의로 선택, 경쟁층의 프로토타입 벡터 와의 거리를 유클리드 거리에 의해 계산
* BMU(Best-Matching Unit) : 표본벡터에 가장 가까운 프로토타입 벡터
* 코호넨의 승자 독점의 학습규칙에 따라 BMU 또는 위상학적 이웃(topological neighbors)에 대한 연결 강도 조정
* 경쟁학습으로 연결강도(connection weight) 반복적으로 재조정하여 학습, 이 과정에서 입력 패턴과 유사한 경쟁층 뉴런이 승자가 됨
* 승자 독식 구조로 인해 경쟁층에는 승자 뉴런만 나타나며, 승자와 유사한 연결 강도를 갖는 입력 패턴이 동일한 경쟁 뉴런으로 배열
* 단 하나의 전방패스(feed-forword flow) 를 사용함으로써 수행속도가 빠르며, 잠재적 실시간 학습처리 가능


<br />

* **SOM 학습 알고리즘**
```
  1. SOM 맵의 노드에 대한 연결강도 초기화
  2. 입력 벡터 제시
  3. (1) 유클리드 거리를 사용하여 입력 벡터와 프로토타입 벡터사이 유사도 계산(Similarity)
     (2) 입력 벡터와 가장 거리가 짧은 프로토타입 벡터(BMU) 탐색
  4. BMU 와 그 이웃들의 연결강도 재조정
  5. 2단계로 반복
```
* **장점**
   * 고차원 데이터를 저차원 지도 형태로 형상화 하므로 시각적으로 이해하기 쉬움
   * 입력 변수 위치 관계가 그대로 보존되므로 실제 데이터가 유사하면 지도상에 가깝게 표현
   * 다양한 연구가 진행되고 패턴 발견, 이미지 분석 등에서 뛰어난 성능  
